- Memory Hierarchy: computers have a few megabytes of very fast, expensive, volatile cache memory, and a few terabytes of slow, cheap, nonvolatile magnetic or solid-state disck storage. 
- Memory Manager: keeps track of which parts of memory are in use, allocate memory to processes when they need it, and deallocate it when they are done. 
	No Memory Abstraction: 
		- Not possible to have two running programs in memory at the same time
		- Every program simply see's physical memory
		- Three ways of organizing physical memory only: 
			1) OS may be at bottom in RAM
			2) OS in ROM at the top
			3) the device drivers may be at the tope of memory in a ROM and the rest of the system in RAM 
		Running Multiple Programs Without a Memory Abstration: 
			- PSW (program status word) is a 4-bit key which allows protection to memory access
			- Static Relocation: when a program is loaded at an address, the constant address value is added to every program 
			address during the load process, which works, but requires extra information in all executable 
			programs to indicate which words contain relocatable addresses and which do not. 
	A Memory Abstration - Address Spaces:
		- Draw backs to exposing physical memory: 
			1) user programs can address every byte of memory, they can easily trash the operating system
			2) it is difficult to have multiple programs running at once
		- The Notion of an Address Space: 
			- Address Space: is the set of addresses that a process can use to address memory
				- Do not have to be numeric can be a set, like internet domains .com
		- Base and Limit Registers: 
			- Dynamic Relocation: maps each process' address space onto a different part of physical memory by using two special
			hardware registers, called base and limit
				- Base Register: When a program is run the base registers is loaded with the length of the program
					- every time a process references memory, either to fetch an instruction or read or write a data word, the
					CPU automatically adds the base value to the address generated by the process before sending the 
					address out on the memory bus
				- Limit register: checks wheher the address offered is equal to or greater than the value in the register, 
				in which case a fault is generated the access is aborted
				- Disadvantage: reloaction using base and limit registerse is the need to perform an addition and a 
				comparison on every memory reference
	Swapping: 
		- Swapping: consists of bringing in each process in its entirety, running it for awhile, 
		so they do not take up any memory when they are not running
		- Virtual Memory: Allows programs to run even when they are only partially in main memory
		- Memory compaction: when swapping creates multiple holes in memory, 
		it is possible to comibne them all into one big one by moving all the processes downward
		- most processes will grow as they run, so it is in our best interest to allocate a little extra memory
		whenever a process is swapped in or moved
			- when this is done we should only move the used memory to the disk and not include the additional space
		- We can give the processes and their components certain space and room, as well as directions, to grow
			- The stack can be at the top that grows downward, data segment shares the same space and grows upward
			- if they run into anothers space the process will need to swap to somewhere with more space or be killed 
	Managing Free Memory: 
		Memory Management with Bitmaps: 
			- Memory is divided into allocation units as small as a few words and as large as several kilobytes
			- Each allocation unit is a bit in the bitmap, which is 0 if the unit is free and 1 if it is occupied
	____  ___  _______   ____   __   _____   ___ ____ 
           |    A ||    ||       B    ||   C  ||   ||   D     || E ||     | processes in memory, empty block is free
	11111 00    111111     1111  00  111111  111  000   bit mapping
	|P|0|5|-|->|H|5|3|-|->|P|8|6|-|->|P|14|4|-|->|H|18|2|-|->|P|20|6|-|->|P|26|3|-|->|H|29|3|x| P - Process, H - hole	
		- Smaller the allocation unit, the larger the bitmap
		- Searching a bitmap for a run of a given length is slow
		Memory Management with Linked Lists: 
			- Linked lists can be leveraged to maintain memory
			- segment list is kept sorted by address, this makes updating the list straightforward and much easier
			- Doubly Linked lists may be better for this implementation
	Before X terminates				After X terminates
		|   A   |   X   |   B   |	becomes	|   A   |       |   B   |
		|   A   |   X   |         |    becomes	|   A   |                 |
		|        |   X   |   B   |     becomes	|	       |   B   |
		|        |   X   |         |    becomes	|		     |
		- List sorted allocation algorithms: 
			-First Fit Algorithm: Memory manager scans along the list of segments until it finds a hole that is big enough, 
			the hole is broen up into two pieces, one for the process and the other for the unused memory
				- fast algorithm
			- Next Fit Algorithm: similar to first fit except that it keeps track of where it is whenever it finds a suitable hole
			next time a hole is needed, it starts searching from the place where it left
			- Best Fit Algorithm: Searches the entire list, from beginning to end, and takes the smallest hole that is adequate, 
			attempting to find the best slot for the given size
				- slow and usually wastes more memory
			- Worst Fit Algorithm: Always take the largest available hole, so that the new hole will be big enough to be useful
			- Quick Fit Algorithm: maintains separate lists for some of the more common sizes requested
				- extremely fast, however determining if you can merge with your neighbor is expensive
	Virtual Memory: 
		- a solution for bloatware that was first discovered was creating overlays, which split programs into little pieces
		- Virtual memory: each program has its own address space, which is broken up into chunks called pages 
			- each page is a contiguous range of addresses 
			- pages are mapped onto physical memory but not all pages have to be in 
			  physical memory at the same time to run the program 
			- when the program references a part of its address space that is in physical memory, 
			  the hardware performs the necessary mappying
				- if not in physical memory, the OS is alerted to get the missing piece and re-execte the instruction that failed
		Paging: 
			-  Virtual addresses: program-generated addresses
			- Virtual addresses do not go directly to the memory bus, they go to the MMU Memory Management Unit, which maps 
			virtual addresses onto the physical memory addresses
			- virtual address space consists of fixed size units and the corresponding units in physical memory are page frames
			- Present/absent bit: keeps track of which pages are phyiscally present in memory
			- Page fault: when the MMU does not detect a mapped page, the CPU traps into the Operating System
				- operating system picks a little-used page frame and writes its contents back to the disk
				 (WHAT IS MEANT BY LITTLE USED PAGE FRAME?) 
				- fetrches from the disk the page that was just referenced into the page from just freed
				- changes the map
				- restarts the trapped instruction
			(NEED THIS PROCESS EXPLAINED) 
			- Page table: page numbers are used to index into, which yields the number of the page frame corresponding to
			 that virtual page 
		Page Tables: 
			- Virtual address is split into a virtual page number (high-order bits) and an offset (low-order bits). 
				- Virtual page number is used as an index into the page table to find the entry for that virtual page
				- Mathematically: Page table is a function, virtual page is an argument, physical frame is the result 
						PT(VP) => PFr
	ex:
		a virtual address is brought in and the first 4 bits are referrencing the page table
		say we have 0010 0000 0000 0100 we will reference 0010 -> 2 in the page table which points to 110 and have a present flag
		that information, 110, is then copied to the outgoing physical address
		the outgoing physical address has 110 and copies the remaining 12 bit offset, so we have: 
			1100 0000 0000 0100

		Structure of a Page Table Enrty: 
			- Most important field is Page frame number

		ex: 	|//////|caching disabled  |referenced  |modified  |protection  |Present/absent  |Page frame number			|
			
			- Present/Absent: if value is 0 then we have a page fault, meaning not in memory, if we have 1 the entry is valid
			- Protection: determines the access permited, 0 is read/write, 1 is read only, can have 3 for determining read, write, exec
			- Modified and Referenced: Track page usage
				- modified: when a page is written to we have a 1, decides if the OS will reclaim the page frame
					- if we do have a 1: it must be written back to memory otherwise it can be abandoned
				- Referenced: set whenever a page is referenced, either for reading or for writing
					- determines when the OS should evict a page due to a page fault 
			- Caching Disabled: allows caching to be disabled for the page 
		Speeding Up Paging: 
			- Two issues are in any paging system: 
				1) Mapping from virtual address to physical address must be fast
					- We want our look up time to be fast so that we do not bottleneck 
				2) If the virtual address space is large, the page table will be large
		Translation Lookaside Buffers: 
			- Most programs tend to make a large number of references to a small number of pages
			- TLB: small hardware device for mapping virtual addresses to physical addresses without going through the page table 
				- inside of the MMU and contains a small number of entries
				- entries contain:
					- information about one page
					- virtual number page
					- bit that is set when page is modified
					- protection code
					- physical page frame in which the page is located 
			- A TLB to speed up paging: 
		Valid		Virtual Page		Modified		Protection		Page Frame
		1		140			1			RW			31
		1		20			0			R   X			38
		1		130			1			RW			29
		1		129			1			RW			62
		1		19			0			R   X			50
		1		21			0			R   X			45
		1		860			1			RW			14
		1		881			1			RW			75
			- when a Virtual address is given to the MMU, 
			the hardware first checks to see if its virtual page number is present in the TLB by comparing it to all the entries in parallel
			- If valid and access protection is not violated, the page frame is take directly from the TLB without going to the page table
			- If protections are violated a page fault occurs
			- When the MMU misses it does an ordinary page lookup and kicks out an entry from the TLB with the one it just looked up
			- When the TLB is loaded from the page table, all the fields are taken from memory
		Software TLB Management: 
			- When a TLB miss occurs, instead of the MMU going to the page tables to find and fetch the needed page reference,
			 it just generates a TLB fault and tosses the problem into the lab of the operating system. 
				- system finds the page, removes the TLB entry, enters the new one, and restarts the instruction that faulted
			-  Strategies for improving performance with TLB management software: 
				1) attack both reducing TLB misses and reducing the cost of TLB miss when it does occur
					a. operating systems can use its intution to figure out which 
					pages are ilkely to be used next and to preoload entries for them in the TLB
			- Normal strategy to process a TLB miss, 
			go to the page table and perform the indexing operations to locate the page referenced
				- problem: pages holding the page table may not be in the TLB, which will cause additional faults 
				- solution: maintain a large software cashe of TLB entries in a fixed location 
			- Soft Miss: When the page referenced is not in the TLB, but is in memory
				- takes about 10-20 machine instructions
			- Hard Miss: Occurs when the page itself is not in memory and not in the TLB, disk access is required
				- extremely slow compared to soft miss
			- Page Table Walk: Looking up mapping in the page table hierarchy
			- Possibilities for misses: 
				1) Minor page fault:
					- the page may actually be in memory, but not in this process' page table
					- no need to access disk again, just map the page appropriately
				2) Major Page fault: 
					- occurs if the page needs to be brought in from the disk
				3) Segmentation fault: 
					- the program simply accessed an invalid address and no mapping needs to be added in the TLB
					- the operating system typically kills the program 
		Page Tables for Large Memories: 
			Multilevel Page Tables: 
				- Keep all the page tables in memory all the time, in particular those that are not needed should not be kept
			Inverted Page Tables: 
				- Inverted Page Tables: one entry per page frame in real memory, 
				   rather than one entry per page of virtual address space
				- downside: virtual to physical translation becomes much harder, requiring a search on every memory reference 
				- We use the TLB to circumvent this process as we can store highly used addresses in the TLB 
				- searching can be simplified by using a hash table in order to search more efficiently
	Page replacement Algorithms: 
		- We want to make sure that we are not evicting pages that are being used frequently
		The Optimal Page Replacement Algorithm: 
			- The page with the highest labe should be removed
				- meaning if one page will not be used for 8 million instructions and another page will not be used in 6 million, 
				removing the former pushes the page fault that will fetch it back as far into the future as possible
			- The OS has no way of knowing when each page of the pages will be referenced next
		The Not Recently Used Page Replacement Algorithm: 
			- Two status bits are used, R and M
				- R: set whenever the page is referenced (read or written)
				- M: set when the page is written to (modified)
				- in every page table entry 
				- hardware sets, OS resets
			- When a process starts both R and M are set to 0
			- Routinely, the R bit is cleared, to distinguish pages that have not been referenced recently from those that have been 
			- When a fault occurs, OS inspects pages and divides them into four categories: 
				1) Class 0: Not referenced, not modified
				2) Class 1: not referenced, modified
				3) referenced, not modified
				4) referenced, modified
			- NRU (Not Recently Used) Algorithm: removes a page at random from the lowest-numbered nonempty class
		First-In, First-Out (FIFO) Page Replacement Algorithm: 
			- OS maintains a list of all pages currently in memory, with the most recent arrival at the tail and the least recent at the head
			- page fault, the page at the head is removed and the new page is added to the tail of the list 
			- the oldest page may still be useful and have very relavent information
		The Second-Change Page Replacement Algorithm: 
			- Is like FIFO but inspects the R bit of the oldest page
				- if 0: page is both old and unused, so replace
				- if 1: the bit is cleared, the page is put onto the end of the list of pages, 
				and it's load time is updated as though it had just arrived in memory 
			- this is known as the second-chance algorithm because if a page is widely used it gets another chance to be in the list
			- this algorithm seeks for a page that is not referenced and is an old page in the most recent clock interval
			- if the algorithm iterates through the entire list, we will default to FIFO and remove the first one
		The Clock Page Replacement Algorithm
			- When a page fault occurs, the page the hand is pointing to is inspected
				- if R = 0: evict the page
				- if R = 1: clear R and advance the hand 
			- repeat the process until a page is found with R=0
		The Least Recently Used (LRU) Page Replacement Algorithm: 
			- When a page fault occurs, throw out the page that has been unused for the longest time
			- Need to maintain a linked list of all pages in memory, with the most recent used page at the front and least in the rear
			- We can implement a counter to each page,
			 so when a page fault occurs we reference the counter and remove the page with the least amount of counts 
		Simulating LRU in Software: 
			- Not Frequently Used (NFU): requires a software counter associated with each page, which is initially zero
				- at each clock interrupt, the OS scans all the counters 
				- the counter keeps track of how many times the page has been referrenced 
				- lowest counter is chosen for replacement
			- We make two modifications to NFU to get LRU: 
				1) the counters are each shifted right 1 bit before the R bit is added in
				2) the R bit is added to the leftmost rather than the rightmost bit 
	R bits for Pages	R bits for Pages	R bits for Pages	R bits for Pages	R bits for Pages
	0-5, clock tick 0	0-5, clock tick 1	0-5, clock tick 2	0-5, clock tick 3	0-5, clock tick 4
	1|0|1|0|1|1		1|1|0|0|1|0		1|1|0|1|0|1		1|0|0|0|1|0		0|1|1|0|0|0
page:
0	1000 0000		1100 0000		1110 0000		1111 0000		0111 1000
1	0000 0000		1000 0000		1100 0000		0110 0000		1011 0000
2	1000 0000		0100 0000		0010 0000		0001 0000		1000 1000
3	0000 0000		0000 0000		1000 0000		0100 0000		0010 0000
4	1000 0000		1100 0000		0110 0000		1011 0000		0101 1000
5	1000 0000		0100 0000		1010 0000		0101 0000		0010 1000

	- we can notice the shift of the bits and the addition of either a 0 or 1 flag on the leftmost bit for each clock tick

		The Working Set Page Replacement Algorithm:
			- Demand Paging: Pages are loaded only on demand, not in advance
			- Locality of reference: during any phase of execution, the process references only a relatively small fraction of its pages
			- Working set: set of pages that a process is currently using is its working set
				- if entire working set is in memory, little to no page faults
				- if too small to fit in memory, the process cause many page faults and runs slow
			- Thrashing: when a program is causing page faults every few instructions
			- Working Set Model: many paging systems try to keep track of each process'
			   working set and make sure that it is in memory before letting the process run 
				- The working set varies slowly with time, it is possible to make a reasonable
				  guess as to which pages will be needed when the program is restarted on the 
				  basis of its working set when it was last stopped. 
				- Working set is pages used in the k most recent memory references and implemented by some prechosed k value
				- Approximations are used, and some based on time, for example: 
				  during the previous 10 million memory references, 
				  we can define it as the set of pages used during the past 100 msec of execution time
					example: process running at time T has 40 msec of CPU time at real time T + 100 msec
					known as Current Virual Time: amount of cpu time a process has actually used since it started
			- Prepaging: loading pages before letting processes run
			- Basic Idea: find apage that is not in the working set and evict
			- Each entry contains two key items: approx time, and R referrence bit

			Current Virtual Time: 		|	2204	|
1:37 PM 4/2/20231:37 PM 4/2/2023
			|	.	|
			|	.	|
			|	.	|
			|___________|
			|___________|
Information	{	|	2084|1|<---- Reference Bit (R)
about 1 page	{	|___________|
			|	2003|1|
			|___________|
Time of last use -------	|------>	1980|1|
			|___________|
			|	1213|0|
Page referenced	|___________|
during this tick --------	|------2014-->|1|
			|___________|
			|	2020|1|		^ Scan all pages examining R bit:
			|___________|		|	if (R == 1) set time of last use to current virtual time
			|	2032|1|		|	if (R == 0 and age > tao) remove page
			|___________|		|	if (R == 0 and age < tao) remember the smallest time
Page not referenced --|------1620-->|0|		|	where tao is seconds of virtual time
during this tick	|___________|		|
			Page Table
			
			Example explination: periodic clock interrupt is assumed to cause software to run that 
			clears the referenced but on every clock tick. if R is 1, current virtual time is written into the 
			Time of last use field in the page table, meaning the page was in use at the time of the fault
			R is 0, the page has not been referenced during the current clock tick and may be a candidate for removal
			The age is compared to time of last use, if it is greater than tao, the page is no longer in the working set 
			and a new page replaces it. 
			R is 0, but age is less than or equal to tao (time of last use), the page is still in the working set
			If all are in the working set, the one that is evicted is the one with the largest age
			Worst case, all pages are referenced and one is randomly removed

		The WSClock Page Replacement Algorithm: 
			- Uses a circular list of page frames
			- Initially empty but populates in a ciruclar fashion and every entry contains a Time of last use field and R bit
			- The algorithm scans using a pointer and if we have a page that is dirty and R = 0 we skip and look for a clean and R = 0
				- also if R = 1, we set it to 0 after the pass 
			- When the hand comes back around to starting point: 
				1) At least one write has been scheduled
					- Hand continues to look for a clean page, because writes are scheduled we will have one soon
				2) No writes have been scheduled
					- All are in the working set, so we claim any clean page and use it, if none exist the current is chosen 
					and written back to disk
		Summary of Page Replacement Algorithm: 
			Algorithm							Comments
		Optimal						Not implementable, but useful as a benchmark
		NRU (Not recently used)				Very crude approximation of LRU
		FIFO(First-in, First-out)				Might throw out important pages
		Second Chance					Big improvement over FIFO
		Clock							Realistic
		LRU (Least Recently Used)				Excellent, but difficult to implement exactly
		NFU (Not frequently used)				Fairly crude approximation of LRU
		Aging							Efficient algorithm that approximates LRU well
		Working Set						Somewhat expensive to implement
		WSClock						Good Efficient Algorithm 
		NOTE: Aging and WSClock are best in practice

	Design Issues for Paging Systems: 
		Local Versus Global Allocation Policies: 
			- Local replacement occurs when a process within memory only references it's own pages for eviction
				- if the working set grows thrashing occurs
				- if the working set shrinks we are wasing memory
			- global replacement occurs when all the process pages are considered for eviction and the one with lowest age is replaced
				-  system must continually decide how many page frames to assign each process
			- PFF (Page fault frequency) algorithm: tells when to increase or decrease a process' page allocation but says nothing about
			which page to replace on a fault
				- this seeks to make sure that all the processes are getting a decent amount of pages so it can mitigate page faults
		Load Control: 
			- Reducing the number of processes competing for memory can be done by swapping some of them to the disk 
			and free up all the pages they are holding
				- this should help with thrashing but if it doesn't another process must be swapped out
		Page Size: 
			- Small page reasoning: 			
				- Internal Fragmentation: a randomly chosen text, data, or stack segment will not fill an internal number of pages, 
				on average, half of the final page will be empty, meaning wasted space
				- using a smaller incrememnt and building up the page size is easier than having one large page size
				- NOTE: more pages are required when the page size is smaller
				-  operating systems sometimes use different page sizes for different parts of the system, 
				large pages for kernels and smaller pages for user processes
		Separate Instruction and Data Spaces: 
			- I-Space and D-Space: Instruction space and data space, address spaces that run from 0 to some maximum
			- When used a linker must be made aware because the D-Space is relocated virtually
		Shared Pages:
			- If we have an I- and D-Space we can have two processes share the same page table for their I-space but different tables
			for their D-space
			-  When two processes share code a problem occurs, because if one process is removed the other still needs the
			recently evicted pages and they must all be generated again because of page faults
			- Copy on Write: Two processes are in Read only, and once a modification is made to either a trap occurs, this trap
			creates a copy made of the requested page and is now has its own private copy, now both are set to Read/write,
			meaning subsequent writes to either copy can be done without trapping
		Share Libraries: 
			- Dynamic Link Libraries: shared libraries that can be utilized by an executable program
			- When linked one or more object files and some libraries are named in the command to the linker 
				ex: ld *.o -lc -lm (links all object files in the current directory and scans two libraries)
			- Undefined externals: any functions called in the object file that are not present are sought in the libraries
				- if found added to the executable binary
			- When the linker is done, an executable binary file is written to the disk containing all the functions needed
			- When program is linked with shared libraries, instead of including the actual function called,
			 the linker includes a small sub routine that binds to the called function at run time	 
			- NOTE: huge advantage to shared libraries - if a function in a shared library is updated to remove a bug, 
			it is not necessary to recompile the programs that call it, the old binaries continue to work
			- Important: the compiler uses a flag that says to not to produce any instructions that use absolute addresses
				- rather use relative addresses only
			- position-independent code: code that uses relative offsets
		Mapped Files: 
			- Memory-mapped files: a process can issue a system call to map a file onto a portion of its virtual address space
			- When the process exits, all the modified pages are writeen back to the file on disk
			- If two or more processes map onto the same file at the same time, they can communicate over shared memory
		Cleaning Policy: 
			- Paging Daemon: ensure a plentiful supply of free page frames, paging systems generally have a background process
			and sleeps most of the time but is awakened periodically to inspect the state of memory
			- Paging daemons ensure that all free frames are clean
			- this is usually done with a two hand clock algorithm where the daemon is in front and investigates the upcoming pages
			it will check if it is dirty, and if so will request for a write back to the disk, and back hand is page replacement 
		Virtual Memory Interface: 
			- Distributed shared memory: allow multiple processes over a network to share a set of pages,
			 as a single shared linear address space 
	
	Implementation Issues: 
		Operating System Involvement with Paging: 
			- Four times when the OS is involved: 
				1) process creation
					- must determine how large the program and data will be and create a page table
				2) process execution
					- MMU has to be reset for te new process and TLB flushed, the new process' page table is made current
				3) page fault
					- read out hardware registeres to determine whihc virtual address caused the fault
				4) process termination
					- must release its page table, its pages, and the disk space that the pages are occupying
		Page Fault Handling: 
			- What happens during a page fault:
			1) hardware traps into the kernel and saves the program counter, sometimes the current state of the instruction is saved
			2) save the general registers and other volatile information
			3) OS discovers the page fault occured, and tries to discover which virtual page is needed
			4) When the location of the fault is known, the system checks to see if this address is valid and the 
			protection is consistent with the access, if a page is free it is used, if not the page replacement algorithm runs
			5) If the page frame is dirty, the page is scheduled for transfer to the disk, context switch takes place, suspending the 
			faulting process and letting another one run until the disk transfer has completed
			6) When clean, the operating system looks up the disk address where the needed page is, and schedules a disk operation
			to bring it in
			7) When the disk interrupt indicates thta the page has arrived, the page tables are updated and reflect its position, 
			the frame is marked as being in a normal state
			8) Faulting instruction is backed up to the state it had when it began and the program counter
			is reset to point to that instruction
			9) faulting process is scheduled, and the operating system returns to the routine that called it
			10) routine reloads the registers nd other state information and returns to user space to continue execution
		Instruction Backup:
			- usually a hidden internal register copies the program counter before each instruction is executed 
			this way the fault location can be determined 
			- some have a second register telling which registers have already been autoincremented/dectremented and by how much
		Locking Pages in Memory: 
			- Pinning: To ensure that I/O processes are not effected heavily by a page fault the pages will be locked in memory so 				that they are not removed by mistake
		Backing Store: 
			- Allocating page space on the disk can be done by the disk having a special swap partition, or sometimes on a separate 
			disk from the file system
		Separation of Policy and Mechanism: 
			- Memory management systems can be can be divided into three parts: 
			1) A low-level MMU handler
				- machine-dependent code and hass to be rewritten for each new plattform it's ported to
			2) A page fault handler that is part of the kernel
				- machine-independent code and contains most of the mechanism for paging
				- gets notified by the external pager where the missing address is located
				- unmaps page from external pager's address space and tasks the MMU handler to put it into the user's address
				space
			3) An external pager running in user space
				- run as a user process
				- notified when processes start in order to set up page map and allocate the necessary backing store on the disk
	Segmentation: 
		- In a one dimensional address space with growing tables, one table may bump into another
		- Segments: provide the machine with many completely independent address spaces
			- consists of a linear sequence of addresses, starting from 0 and going up to some maximum value
			- size may change during execution
			- might contain procedure, arrray, stack, or a collection of scalar variables, but usually does not contain a mixture of
			 different types
		- Facilitates sharing procedures or data between several processes
		Implemetation of Pure Segmetation: 
			- Differs from paging because pages are of fixed size and segments are not
 			- External fragmentation/Checkerboarding: when memory will be divided up into a number of chunks,
			 some containing segments and some containing holes
		Segmentation with Paging:  MULTICS
			Comparing segmentation and paging: 
		Consideration								Paging				Segmentation
		Need the programmer be aware that this technique is being used? No				Yes
		How many linear address spaces are there?				1				Many
		Can the total address space exceed the size of physical memory? 	Yes				Yes	
		Can procedures and data be distinguished 				No				Yes
		and separately protected?
		Can tables whose siize fluctuates be accommodated easily?	No				Yes
		Is sharing of procedures between users facilitated?			No				Yes
		Why was this technique invented?					To get large linear address	To allow programs and data 
											space without having to buy   to be broken up into logically
											more physical memory	independent address spaces 
															and to aid sharing and 
															protection